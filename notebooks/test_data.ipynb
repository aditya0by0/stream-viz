{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0128237f-31f4-4597-be91-3d199483c7da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, argparse\n",
    "\n",
    "# sys.path.append(os.path.join(os.getcwd(), '.'))\n",
    "\n",
    "# import additions.constants as consts\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40658c70-a2c2-4349-80a5-b90628e234ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1) (1000, 1) (1000, 1) (1000, 1) (1000, 1)\n",
      "(1000, 1) (1000, 1) (1000, 1) (1000, 1) (1000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N0</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.505321</td>\n",
       "      <td>0.284864</td>\n",
       "      <td>0.655726</td>\n",
       "      <td>0.150357</td>\n",
       "      <td>0.395861</td>\n",
       "      <td>c</td>\n",
       "      <td>C</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.409260</td>\n",
       "      <td>0.031902</td>\n",
       "      <td>0.320296</td>\n",
       "      <td>0.653137</td>\n",
       "      <td>0.331237</td>\n",
       "      <td>b</td>\n",
       "      <td>B</td>\n",
       "      <td>d</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.904029</td>\n",
       "      <td>0.746586</td>\n",
       "      <td>1.328019</td>\n",
       "      <td>0.417862</td>\n",
       "      <td>0.838134</td>\n",
       "      <td>c</td>\n",
       "      <td>C</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063662</td>\n",
       "      <td>0.621714</td>\n",
       "      <td>1.291321</td>\n",
       "      <td>0.526868</td>\n",
       "      <td>0.456212</td>\n",
       "      <td>e</td>\n",
       "      <td>E</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740151</td>\n",
       "      <td>0.773944</td>\n",
       "      <td>1.454796</td>\n",
       "      <td>0.072190</td>\n",
       "      <td>0.444516</td>\n",
       "      <td>d</td>\n",
       "      <td>E</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         N0        N1        N2        N3        N4 C5 C6 C7 C8 C9\n",
       "0  0.505321  0.284864  0.655726  0.150357  0.395861  c  C  a  b  d\n",
       "1  0.409260  0.031902  0.320296  0.653137  0.331237  b  B  d  b  a\n",
       "2  0.904029  0.746586  1.328019  0.417862  0.838134  c  C  c  d  a\n",
       "3  0.063662  0.621714  1.291321  0.526868  0.456212  e  E  c  c  d\n",
       "4  0.740151  0.773944  1.454796  0.072190  0.444516  d  E  a  b  b"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_mixed_dataset():\n",
    "    # folder = os.path.join(consts.DIR_CSV, 'corr', 'mixed')\n",
    "    # filename = os.path.join(folder, 'raw_data.pkl.gzip')\n",
    "    # if not os.path.exists(folder):\n",
    "    #     os.mkdir(folder)\n",
    "\n",
    "    # 8 features, 4 num, 4 cat (n1, n2, n3, n4, c5, c6, c7, c8)\n",
    "    # n0 â‚¬ [0..1] Useless feature\n",
    "    # n1 â‚¬ [0..1]\n",
    "    # n2 â‚¬ 2 * n1 Â± N(0, 0.2)\n",
    "    # n3 â‚¬ [0..1]\n",
    "    # n4 â‚¬ f(c8) -> [a: 0.1, b: 0.3, c: 0.5, d: 0.7, e: 0.9] Â± N(0, 0.2)\n",
    "    # c5 â‚¬ [a, b, c, d, e]\n",
    "    # c6 â‚¬ upper(c5) with 20% chance of random change to another category\n",
    "    # c7 â‚¬ f(n3) -> {a:0..<0.2, b:0.2..<0.4, c:0.4..<0.6, d:0.6..<0.8, e:0.8..1.0} with 20% chance of random change to another category\n",
    "    # c8 â‚¬ [a, b, c, d, e]\n",
    "    # c9 â‚¬ [a, b, c, d, e] Useless feature\n",
    "\n",
    "    d = 1000\n",
    "    r = 0.2\n",
    "\n",
    "    n0 = np.random.random((d, 1))\n",
    "    n1 = np.random.random((d, 1))\n",
    "    n2 = n1 * 2 + np.random.normal(0, r, (d, 1))\n",
    "    n3 = np.random.random((d, 1))\n",
    "\n",
    "    def ccol(ra=None):\n",
    "        c = np.array([[\"\"] * d]).T\n",
    "        if ra is None:\n",
    "            ra = np.random.random((d, 1))\n",
    "        np.putmask(c, ra < 0.2, \"a\")\n",
    "        np.putmask(c, (ra >= 0.2) & (ra < 0.4), \"b\")\n",
    "        np.putmask(c, (ra >= 0.4) & (ra < 0.6), \"c\")\n",
    "        np.putmask(c, (ra >= 0.6) & (ra < 0.8), \"d\")\n",
    "        np.putmask(c, ra > 0.8, \"e\")\n",
    "        return c\n",
    "\n",
    "    c5 = ccol()\n",
    "    c6 = c5.copy()\n",
    "    np.putmask(c6, np.random.random((d, 1)) < 0.2, ccol())\n",
    "    c6[:, 0] = np.apply_along_axis(lambda x: x[0].upper(), 1, c6)\n",
    "    c7 = ccol(ra=n3)\n",
    "    np.putmask(c7, np.random.random((d, 1)) < 0.2, ccol())\n",
    "    c8 = ccol()\n",
    "    c9 = ccol()\n",
    "\n",
    "    n4 = np.zeros((d, 1))\n",
    "    np.putmask(n4, c8 == \"a\", 0.1)\n",
    "    np.putmask(n4, c8 == \"b\", 0.3)\n",
    "    np.putmask(n4, c8 == \"c\", 0.5)\n",
    "    np.putmask(n4, c8 == \"d\", 0.7)\n",
    "    np.putmask(n4, c8 == \"e\", 0.9)\n",
    "    n4 += np.random.normal(0, r, (d, 1))\n",
    "\n",
    "    print(n0.shape, n1.shape, n2.shape, n3.shape, n4.shape)\n",
    "    print(c5.shape, c6.shape, c7.shape, c8.shape, c9.shape)\n",
    "\n",
    "    data = np.concatenate([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9], axis=1)\n",
    "    cols = [\"N0\", \"N1\", \"N2\", \"N3\", \"N4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\"]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "    df[cols[:5]] = df[cols[:5]].astype(float)\n",
    "    # df.to_pickle(filename)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "gen_mixed_dataset().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c88d94e-bdec-4558-b9ca-c94ea3e58109",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0. ,  0. ,  2.5,  5. ,  7.5, 10. , 10. , 10. , 10. ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------- Linear Drift -------------------------\n",
    "def _lin_incr(begin, end, i_begin, i_end, chunk):\n",
    "    \"\"\"\n",
    "    Linear Drift : Linearly increments the values\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    begin = 0\n",
    "    end = 10\n",
    "    i_begin = 2\n",
    "    i_end = 5\n",
    "    chunk = 10\n",
    "    Calculation:\n",
    "\n",
    "    vals = 5 - 2 + 1 = 4\n",
    "    srange = 10 - 0 = 10\n",
    "    step = 10 / 4 = 2.5\n",
    "    moving_values = np.arange(0, 10, 2.5) = [0.0, 2.5, 5.0, 7.5]\n",
    "    Concatenation:\n",
    "\n",
    "    [0]*2 = [0, 0] (first 2 elements are 0)\n",
    "    moving_values = [0.0, 2.5, 5.0, 7.5] (linearly spaced values)\n",
    "    [10]*(10 - 5 - 1) = [10, 10, 10, 10] (remaining elements are 10)\n",
    "    Result:\n",
    "\n",
    "    The final array is [0, 0, 0.0, 2.5, 5.0, 7.5, 10, 10, 10, 10]\n",
    "    \"\"\"\n",
    "    # number of values to generate, inclusive of the beginning index OR end index\n",
    "    vals = i_end - i_begin + 1  # 5 - 2 + 1 = 4 -------- this is from moving values\n",
    "    # range of values to generate from\n",
    "    srange = end - begin  # generate values from a range of 0 to 10 ....(10-0)\n",
    "    # from the range of the values, and the number of values to generate,\n",
    "    # derive the step size, inorder to generate the no of values given in given range\n",
    "    step = srange / vals  # ------------- this is for moving valuez\n",
    "    # moving_values = np.arange(begin, end * (1 + 1 / (vals + 1)), step)\n",
    "    moving_values = np.arange(begin, end, step)\n",
    "    return np.append(\n",
    "        np.append(\n",
    "            [begin] * i_begin, moving_values\n",
    "        ),  # combine begin value, \"i_begin\" number of times,\n",
    "        # before even starting with, the values begin, then after that append the moving values\n",
    "        [end]\n",
    "        * (\n",
    "            chunk - i_end - 1\n",
    "        ),  # now add the end values till with remaining size indicated by chunk\n",
    "    )\n",
    "    # result : [0, 0, 0.0, 2.5, 5.0, 7.5, 10, 10, 10, 10]\n",
    "    # gives a linear increment in values\n",
    "\n",
    "\n",
    "_lin_incr(begin=0, end=10, i_begin=2, i_end=5, chunk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972b1a57-6404-4677-98b8-20647c37e040",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ Blend two concepts gradually over a specified range -----------------\n",
    "def _gradual(concept1, concept2, i_begin, i_end, chunk):\n",
    "    \"\"\"\n",
    "    The function _gradual is designed to blend two concepts (arrays of values) gradually over a specified range of indices.\n",
    "\n",
    "    Parameters:\n",
    "    concept1: The first concept, which can be a NumPy array, a pandas Series, or a single value.\n",
    "    concept2: The second concept, which can also be a NumPy array, a pandas Series, or a single value.\n",
    "    i_begin: The starting index of the range over which the blending occurs.\n",
    "    i_end: The ending index of the range over which the blending occurs.\n",
    "    chunk: The size of the arrays that will be generated if the concepts are not already arrays.\n",
    "    \"\"\"\n",
    "    if not isinstance(concept1, np.ndarray):\n",
    "        if isinstance(concept1, pd.Series):\n",
    "            concept1 = concept1.to_numpy()\n",
    "        else:\n",
    "            concept1 = np.array([concept1] * chunk)\n",
    "    if not isinstance(concept2, np.ndarray):\n",
    "        if isinstance(concept1, pd.Series):\n",
    "            concept2 = concept2.to_numpy()\n",
    "        else:\n",
    "            concept2 = np.array([concept2] * chunk)\n",
    "    # The function _lin_incr (presumably defined elsewhere) generates an array called chances which linearly\n",
    "    # increases from 0 to 1 over the specified range from i_begin to i_end, with a length of chunk.\n",
    "    chances = _lin_incr(0, 1, i_begin, i_end, chunk)\n",
    "    # Create a mask based on random values and the chance array:\n",
    "    mask = np.random.random(chunk) < chances\n",
    "    t = concept1.copy()\n",
    "    # Changes elements of an array based on conditional and input values.\n",
    "    # ------------ Blend two concepts gradually over a specified range -----------------\n",
    "    # ------------ Blend two concepts gradually over a specified range -----------------\n",
    "    # ------------ Blend two concepts gradually over a specified range -----------------\n",
    "    # t = [1, 2, 3, 4], mask = [true, false, true , false], concept2 = [10, 8, 7, 6]\n",
    "    # t= [1, 8, 3, 6]\n",
    "    # ------------ Blend two concepts gradually over a specified range -----------------\n",
    "    np.putmask(t, mask, concept2)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be75b30c-520d-452e-8942-43cc9dea292f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def change_label_func(filepath, class_col=\"class\"):\n",
    "    df = pd.read_pickle(filepath)\n",
    "\n",
    "    def re_class(\n",
    "        b: int,\n",
    "        e: int,\n",
    "        ccol: str,\n",
    "        true_ncols: list,\n",
    "        false_ncols: list,\n",
    "        true_coefs: list,\n",
    "        false_coefs: list,\n",
    "        thrs: list,\n",
    "        revs: list,\n",
    "        flip_chance: float = 0.0,\n",
    "        df: pd.DataFrame = df,\n",
    "        lcol: str = class_col,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "\n",
    "        b, e: Start and end indices of the rows to be processed.\n",
    "        ccol: Column name or array indicating a specific condition (e.g., \"c7\").\n",
    "        true_ncols, false_ncols: Column names used for the condition checks.\n",
    "        true_coefs, false_coefs: Coefficients applied to the columns for the condition checks.\n",
    "        thrs: Threshold values for the conditions.\n",
    "        revs: Booleans to reverse the condition logic.\n",
    "        flip_chance: Probability of flipping the label.\n",
    "        df: DataFrame to be processed.\n",
    "        lcol: The label column to be updated.\n",
    "        \"\"\"\n",
    "        # b = 0, e = 999 => 1000\n",
    "        t = np.zeros(e - b + 1).astype(\"bool\")\n",
    "        temp = np.zeros(e - b + 1)\n",
    "\n",
    "        #  ccol: str = \"c7\",\n",
    "        # true_ncols: list = [\"n2\", \"n3\"],\n",
    "        # false_ncols: list = [\"n3\", \"n4\"],\n",
    "        # true_coefs: list =  [1, 1]\n",
    "        # false_coefs: list = [1, 1]\n",
    "        # thres = [0.5, -0.5],\n",
    "        # revs = [False, False],\n",
    "        # flip chance = 1/32\n",
    "\n",
    "        for ncol, coef in zip(true_ncols, true_coefs):\n",
    "            temp += df.loc[b:e, ncol] * coef\n",
    "        temp = (temp < thrs[0]) ^ revs[0]\n",
    "        if isinstance(ccol, str):\n",
    "            temp = (df.loc[b:e, ccol] == \"b\") & temp\n",
    "        else:\n",
    "            temp = (ccol == \"b\") & temp\n",
    "        t = t | temp\n",
    "\n",
    "        for ncol, coef in zip(false_ncols, false_coefs):\n",
    "            temp += df.loc[b:e, ncol] * coef\n",
    "        temp = (temp < thrs[1]) ^ revs[1]\n",
    "        if isinstance(ccol, str):\n",
    "            temp = (df.loc[b:e, ccol] != \"b\") & temp\n",
    "        else:\n",
    "            temp = (ccol != \"b\") & temp\n",
    "        t = t | temp\n",
    "\n",
    "        flip = np.random.random(e - b + 1) < flip_chance\n",
    "        print(flip_chance, flip.sum())\n",
    "        df.loc[b:e, lcol] = t ^ flip\n",
    "\n",
    "    re_class(\n",
    "        0,\n",
    "        9999,\n",
    "        \"c7\",\n",
    "        [\"n2\", \"n3\"],\n",
    "        [\"n3\", \"n4\"],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, -0.5],\n",
    "        [False, False],\n",
    "        1 / 32,\n",
    "    )\n",
    "    re_class(\n",
    "        10000,\n",
    "        10999,\n",
    "        \"c7\",\n",
    "        [\"n2\", \"n3\"],\n",
    "        [\"n3\", \"n4\"],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [_lin_incr(0.5, -1, 0, 499, 1000), _lin_incr(-0.5, 0, 0, 499, 1000)],\n",
    "        [False, False],\n",
    "        1 / 32,\n",
    "    )\n",
    "    grad = _gradual(df.loc[11000:11999, \"c7\"], df.loc[11000:11999, \"c6\"], 0, 499, 1000)\n",
    "    grad = pd.Series(grad, index=range(11000, 12000))\n",
    "    re_class(\n",
    "        11000,\n",
    "        11999,\n",
    "        grad,\n",
    "        [\"n2\", \"n3\"],\n",
    "        [\"n3\", \"n4\"],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [-1, 0],\n",
    "        [False, False],\n",
    "        1 / 32,\n",
    "    )\n",
    "    re_class(\n",
    "        12000,\n",
    "        12999,\n",
    "        \"c5\",\n",
    "        [\"n0\", \"n4\"],\n",
    "        [\"n1\"],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0.5],\n",
    "        [False, True],\n",
    "        1 / 32,\n",
    "    )\n",
    "\n",
    "    df[class_col] = df[class_col].replace({0: \"A\", 1: \"B\"})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a4061f-1280-4de6-9886-1989293c7790",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def change_label_func_splits(filepath, class_col=\"class\"):\n",
    "    df = pd.read_pickle(filepath)\n",
    "    df[class_col] = df[class_col].replace({\"A\": 0, \"B\": 1})\n",
    "    folder = os.path.join(os.path.dirname(filepath), \"prepared\")\n",
    "\n",
    "    for i in range(30):\n",
    "        for m in [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875]:\n",
    "            datapath = os.path.join(folder, str(i), str(m), \"labeled_set.pkl.gzip\")\n",
    "            data = pd.read_pickle(datapath)\n",
    "            data.loc[data.index, class_col] = df.loc[data.index, class_col]\n",
    "            data.to_pickle(datapath)\n",
    "            for num in range(0, 12950, 50):\n",
    "                datapath = os.path.join(folder, str(i), str(m), f\"data{num}.pkl.gzip\")\n",
    "                data = pd.read_pickle(datapath)\n",
    "                data.loc[data.index, class_col] = df.loc[data.index, class_col]\n",
    "                data.to_pickle(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d007d4-942c-4963-82f5-82258065746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncol(stddev=1, ra=None, ra2=None, mult_ra=1, mult_ra2=1):\n",
    "    \"\"\"\n",
    "    The function ncol is designed to generate a sequence of numbers with an optional noise component.\n",
    "\n",
    "    Parameters\n",
    "    --------------------------------------------------------------\n",
    "\n",
    "    stddev: default = 1\n",
    "            This parameter controls the standard deviation of the noise to be added.\n",
    "            A higher value results in more variability (noise) in the generated sequence.\n",
    "            If stddev is set to 0, no noise is added.\n",
    "\n",
    "    ra: default = None\n",
    "        This is an optional parameter. default = None\n",
    "        If provided, it is used as a base sequence to which the noise and other modifications are applied.\n",
    "\n",
    "    ra2: default = None\n",
    "         This is another optional parameter.\n",
    "         If provided, it is used alongside ra to further modify the generated sequence.\n",
    "\n",
    "    mult_ra: default = 1\n",
    "             This multiplier is applied to the ra sequence if ra is provided.\n",
    "\n",
    "    mult_ra2: default = 1\n",
    "              This multiplier is applied to the ra2 sequence if ra2 is provided.\n",
    "    \"\"\"\n",
    "    if stddev == 0:\n",
    "        noise = np.zeros(\n",
    "            chunk\n",
    "        )  # If stddev is 0, noise is an array of zeros with length chunk.\n",
    "    else:\n",
    "        # Otherwise noise is generated as a sequence of random numbers from a normal distribution with the specified standard deviation stddev.\n",
    "        noise = np.random.normal(scale=stddev, size=chunk)\n",
    "    # else: noise = np.random.uniform(high=stddev, size=chunk)\n",
    "    if ra is None:\n",
    "        return noise\n",
    "    elif ra2 is None:\n",
    "        return mult_ra * (ra + noise)\n",
    "    else:\n",
    "        return (mult_ra * ra) + (mult_ra2 * ra2) + noise\n",
    "\n",
    "\n",
    "chunk = 1000  # size of array to return\n",
    "err = 0.125  # err is set to 0.125, which will be used as the standard deviation for noise in the second call to ncol.\n",
    "n1 = (\n",
    "    ncol()\n",
    ")  # This generates n1 as a sequence of 1000 random numbers from a normal distribution with a standard deviation of 1.\n",
    "# This generates n2 as a sequence where each element is computed as 2 Ã— (ð‘›1[ð‘–] + noise[ð‘–]),\n",
    "# where noise is a new sequence of random numbers from a normal distribution with a standard deviation of 0.125.\n",
    "n2 = ncol(err, n1, mult_ra=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a46fa821-3138-48c7-83d7-8186c47be9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccol(flip_chance=0, ra=None, ra2=None, ra_thr=0, reverse_cats=False):\n",
    "    \"\"\"\n",
    "    The ccol function generates a binary array (with values True or False) based on various conditions and parameters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------------------------------------------\n",
    "    flip_chance: Probability (between 0 and 1) of flipping each value in the array.\n",
    "    ra: An optional array or value to determine the condition.\n",
    "    ra2: A second optional array for logical operations.\n",
    "    ra_thr: A threshold value for comparisons.\n",
    "    reverse_cats: If True, reverses the values in the resulting array.\n",
    "    \"\"\"\n",
    "    if ra is None:\n",
    "        ra = np.random.random(chunk)\n",
    "        c = ra < ra_thr\n",
    "    elif ra2 is None:\n",
    "        if np.issubdtype(ra.dtype, np.number):\n",
    "            c = ra < ra_thr\n",
    "        else:\n",
    "            c = ra\n",
    "    else:\n",
    "        c = np.logical_xor(ra, ra2)\n",
    "\n",
    "    flip = np.random.random(chunk) < flip_chance\n",
    "    c = np.logical_xor(c, flip)  # true only when one of the value is TRUE\n",
    "    if reverse_cats:\n",
    "        c = ~c\n",
    "    return c\n",
    "\n",
    "\n",
    "dataset = np.zeros((0, 11))\n",
    "chunk = 1000\n",
    "t_end = chunk // 2 - 1\n",
    "err = 0.125\n",
    "c5 = ccol(ra_thr=0.5)\n",
    "c6 = ccol(ra_thr=0.5)\n",
    "c7 = ccol(err, c6)\n",
    "# c8 = ccol(err, n3, ra_thr=0)\n",
    "# c9 = ccol(err, c6, c8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3087fd52-ed96-4611-b54a-77c6b718b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfpdss_dataset():\n",
    "    folder = os.path.join(\"consts.DIR_CSV\", \"cfpdss\")\n",
    "    filename = os.path.join(folder, \"raw_data.pkl.gzip\")\n",
    "    # 10 features, 1 label 2 classes\n",
    "    # n0 â‚¬ N1\n",
    "    # n1 â‚¬ N1\n",
    "    # n2 â‚¬ a * (n1 Â± N.25)\n",
    "    # n3 â‚¬ N1\n",
    "    # n4 â‚¬ a * n1 + (1 - a) * n3\n",
    "    # c5 â‚¬ [a, b, c, d], no correlation\n",
    "    # c6 â‚¬ [a, b, c, d]\n",
    "    # c7 â‚¬ c5 with 25% chance to reroll random category\n",
    "    # c8 â‚¬ f(n3) -> {a:0..<0.25, b:0.25..<0.5, c:0.5..<0.75, d:0.75..<1.0} with 25% chance to reroll random category\n",
    "    # c9 â‚¬ [a, b, c, d]\n",
    "    # l â‚¬ {0, 1} c7 n2\n",
    "\n",
    "    dataset = np.zeros((0, 11))\n",
    "    chunk = 1000\n",
    "    t_end = chunk // 2 - 1\n",
    "    err = 0.125\n",
    "\n",
    "    def lin_incr(begin, end, i_begin=0, i_end=t_end):\n",
    "        return _lin_incr(begin, end, i_begin, i_end, chunk)\n",
    "\n",
    "    def gradual(concept1, concept2, i_begin=0, i_end=t_end):\n",
    "        return _gradual(concept1, concept2, i_begin, i_end, chunk)\n",
    "\n",
    "    def tfunc(\n",
    "        ccol1,\n",
    "        ccol2,\n",
    "        ncols1,\n",
    "        ncols2,\n",
    "        ncols3,\n",
    "        ncols4,\n",
    "        coefs1,\n",
    "        coefs2,\n",
    "        coefs3,\n",
    "        coefs4,\n",
    "        thrs,\n",
    "        revs,\n",
    "        flip_chance=0,\n",
    "    ):\n",
    "        t = np.zeros(chunk).astype(\"bool\")\n",
    "\n",
    "        temp = np.zeros(chunk)\n",
    "        for ncol, coef in zip(ncols1, coefs1):\n",
    "            temp += ncol * coef\n",
    "        temp = ccol1 & ccol2 & ((temp < thrs[0]) ^ revs[0])\n",
    "        t = t | temp\n",
    "\n",
    "        temp = np.zeros(chunk)\n",
    "        for ncol, coef in zip(ncols2, coefs2):\n",
    "            temp += ncol * coef\n",
    "        temp = ccol1 & (~ccol2) & ((temp < thrs[1]) ^ revs[1])\n",
    "        t = t | temp\n",
    "\n",
    "        temp = np.zeros(chunk)\n",
    "        for ncol, coef in zip(ncols3, coefs3):\n",
    "            temp += ncol * coef\n",
    "        temp = (~ccol1) & ccol2 & ((temp < thrs[2]) ^ revs[2])\n",
    "        t = t | temp\n",
    "\n",
    "        temp = np.zeros(chunk)\n",
    "        for ncol, coef in zip(ncols4, coefs4):\n",
    "            temp += ncol * coef\n",
    "        temp = (~ccol1) & (~ccol2) & ((temp < thrs[3]) ^ revs[3])\n",
    "        t = t | temp\n",
    "\n",
    "        flip = np.random.random(chunk) < flip_chance\n",
    "        return np.logical_xor(t, flip)\n",
    "\n",
    "    def ncol(stddev=1, ra=None, ra2=None, mult_ra=1, mult_ra2=1):\n",
    "        if stddev == 0:\n",
    "            noise = np.zeros(chunk)\n",
    "        else:\n",
    "            noise = np.random.normal(scale=stddev, size=chunk)\n",
    "        # else: noise = np.random.uniform(high=stddev, size=chunk)\n",
    "        if ra is None:\n",
    "            return noise\n",
    "        elif ra2 is None:\n",
    "            return mult_ra * (ra + noise)\n",
    "        else:\n",
    "            return (mult_ra * ra) + (mult_ra2 * ra2) + noise\n",
    "\n",
    "    def ccol(flip_chance=0, ra=None, ra2=None, ra_thr=0, reverse_cats=False):\n",
    "        if ra is None:\n",
    "            ra = np.random.random(chunk)\n",
    "            c = ra < ra_thr\n",
    "        elif ra2 is None:\n",
    "            if np.issubdtype(ra.dtype, np.number):\n",
    "                c = ra < ra_thr\n",
    "            else:\n",
    "                c = ra\n",
    "        else:\n",
    "            c = np.logical_xor(ra, ra2)\n",
    "\n",
    "        flip = np.random.random(chunk) < flip_chance\n",
    "        c = np.logical_xor(c, flip)\n",
    "        if reverse_cats:\n",
    "            c = ~c\n",
    "        return c\n",
    "\n",
    "    # n0 = ncol()\n",
    "    # n1 = ncol()\n",
    "    # n2 = ncol(0, n1, mult_ra=2)\n",
    "    # n3 = ncol()\n",
    "    # n4 = ncol(0, n1, n3, .5, .5)\n",
    "    # c5 = ccol(ra_thr=.5)\n",
    "    # c6 = ccol(ra_thr=.5)\n",
    "    # c7 = ccol(0, c6)\n",
    "    # c8 = ccol(0, n3, ra_thr=0)\n",
    "    # c9 = ccol(0, c6, c8)\n",
    "    # t = tfunc(c7, c7, [n2], [n2], [n2], [n2], [1], [1], [1], [1],\n",
    "    #     [.5, 0, 0, .5], [False, False, False, True]) #c7:n2<.5, ~c7:n2>.5\n",
    "    # tenbatch = np.concatenate([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t], axis=1)\n",
    "    # dataset = np.concatenate([dataset, tenbatch])\n",
    "    # pd.DataFrame(dataset).to_csv('bla.csv', sep='\\t')\n",
    "    # print(dataset)\n",
    "    # #print('0',n0,'1',n1,'2',n2,'3',n3,'4',n4,'5',c5,'6',c6,'7',c7,'8',c8,'9',c9,'C',t)\n",
    "    # raise\n",
    "\n",
    "    # 0..999\n",
    "    n0 = ncol()\n",
    "    n1 = ncol()\n",
    "    n2 = ncol(err, n1, mult_ra=2)\n",
    "    n3 = ncol()\n",
    "    n4 = ncol(err, n1, n3, 0.5, 0.5)\n",
    "    c5 = ccol(ra_thr=0.5)\n",
    "    c6 = ccol(ra_thr=0.5)\n",
    "    c7 = ccol(err, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=0)\n",
    "    c9 = ccol(err, c6, c8)\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 1000..1999 inc num f\n",
    "    n0 = ncol() + lin_incr(0, 2)  # to 2\n",
    "    n1 = ncol() + lin_incr(0, 1)  # to 1\n",
    "    n2 = ncol(err, n1, mult_ra=2)\n",
    "    n3 = ncol() + lin_incr(0, -1)  # to -1\n",
    "    n4 = ncol(err, n1, n3, 0.5, 0.5)\n",
    "    c5 = ccol(ra_thr=0.5)\n",
    "    c6 = ccol(ra_thr=0.5)\n",
    "    c7 = ccol(err, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=0)\n",
    "    c9 = ccol(err, c6, c8)\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 2000..2999 inc cat f\n",
    "    n0 = ncol() + 2\n",
    "    n1 = ncol() + 1\n",
    "    n2 = ncol(err, n1, mult_ra=2)\n",
    "    n3 = ncol() - 1\n",
    "    n4 = ncol(err, n1, n3, 0.5, 0.5)\n",
    "    c5 = ccol(ra_thr=lin_incr(0.5, 0.8))  # to .8\n",
    "    c6 = ccol(ra_thr=lin_incr(0.5, 0.2))  # to .2\n",
    "    c7 = ccol(err, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=lin_incr(0, -1))  # to -1\n",
    "    c9 = ccol(err, c6, c8)\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 3000..3999 inc coef f\n",
    "    n0 = ncol() + 2\n",
    "    n1 = ncol() + 1\n",
    "    n2 = ncol(err, n1, mult_ra=lin_incr(2, 5))  # to 5\n",
    "    n3 = ncol() - 1\n",
    "    n4 = ncol(err, n1, n3, lin_incr(0.5, 0.8), lin_incr(0.5, 0.2))  # to (.8, .2)\n",
    "    c5 = ccol(ra_thr=0.8)\n",
    "    c6 = ccol(ra_thr=0.2)\n",
    "    c7 = ccol(lin_incr(err, 0.25), c6)  # to .25\n",
    "    c8 = ccol(err, n3, ra_thr=-1)\n",
    "    c9 = ccol(lin_incr(err, 0.25), c6, c8)  # to .25\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 4000..4999 gra num f\n",
    "    n0 = ncol() + gradual(2, -2)  # to -2\n",
    "    n1 = ncol() + gradual(1, -1)  # to -1\n",
    "    n2 = ncol(err, n1, mult_ra=5)\n",
    "    n3 = ncol() + gradual(-1, 1)  # to 1\n",
    "    n4 = ncol(err, n1, n3, 0.8, 0.2)\n",
    "    c5 = ccol(ra_thr=0.8)\n",
    "    c6 = ccol(ra_thr=0.2)\n",
    "    c7 = ccol(0.25, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=-1)\n",
    "    c9 = ccol(0.25, c6, c8)\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 5000..5999 gra cat f\n",
    "    n0 = ncol() - 2\n",
    "    n1 = ncol() - 1\n",
    "    n2 = ncol(err, n1, mult_ra=5)\n",
    "    n3 = ncol() + 1\n",
    "    n4 = ncol(err, n1, n3, 0.8, 0.2)\n",
    "    c5 = ccol(ra_thr=gradual(0.8, 0.2))  # to .2\n",
    "    c6 = ccol(ra_thr=gradual(0.2, 0.8))  # to .8\n",
    "    c7 = ccol(0.25, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=gradual(-1, 1))  # to 1\n",
    "    c9 = ccol(0.25, c6, c8)\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 6000..6999 gra coef f\n",
    "    n0 = ncol() - 2\n",
    "    n1 = ncol() - 1\n",
    "    n2 = ncol(err, n1, mult_ra=gradual(5, -3))  # to -3\n",
    "    n3 = ncol() + 1\n",
    "    n4 = ncol(err, n1, n3, gradual(0.8, 0.2), gradual(0.2, 0.8))  # to (.2, .8)\n",
    "    c5 = ccol(ra_thr=0.2)\n",
    "    c6 = ccol(ra_thr=0.8)\n",
    "    c7 = ccol(gradual(0.25, 0.375), c6)  # to .375\n",
    "    c8 = ccol(err, n3, ra_thr=1)\n",
    "    c9 = ccol(0.25, gradual(c6, c5), c8)  # to c5\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 7000..7999 sudden num f\n",
    "    n0 = ncol()  # to 0\n",
    "    n1 = ncol()  # to 0\n",
    "    n2 = ncol(err, n1, mult_ra=-3)\n",
    "    n3 = ncol()  # to 0\n",
    "    n4 = ncol(err, n1, n3, 0.2, 0.8)\n",
    "    c5 = ccol(ra_thr=0.2)\n",
    "    c6 = ccol(ra_thr=0.8)\n",
    "    c7 = ccol(0.375, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=1)\n",
    "    c9 = ccol(0.25, c5, c8)\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 8000..8999 sudden num f\n",
    "    n0 = ncol()\n",
    "    n1 = ncol()\n",
    "    n2 = ncol(err, n1, mult_ra=-3)\n",
    "    n3 = ncol()\n",
    "    n4 = ncol(err, n1, n3, 0.2, 0.8)\n",
    "    c5 = ccol(ra_thr=0.5)  # to .5\n",
    "    c6 = ccol(ra_thr=0.5)  # to .5\n",
    "    c7 = ccol(0.375, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=0)  # to 0\n",
    "    c9 = ccol(0.25, c5, c8)\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 9000..9999 sudden coef f\n",
    "    n0 = ncol()\n",
    "    n1 = ncol()\n",
    "    n2 = ncol(err, n1, mult_ra=2)  # to 2\n",
    "    n3 = ncol()\n",
    "    n4 = ncol(err, n1, n3, 0.5, 0.5)  # to (.5, .5)\n",
    "    c5 = ccol(ra_thr=0.5)\n",
    "    c6 = ccol(ra_thr=0.5)\n",
    "    c7 = ccol(err, c6)  # to .125\n",
    "    c8 = ccol(err, n3, ra_thr=0)\n",
    "    c9 = ccol(err, c6, c8)  # to .125, c6\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [0.5, 0, 0, 0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 10000..10999 incremental label\n",
    "    n0 = ncol()\n",
    "    n1 = ncol()\n",
    "    n2 = ncol(err, n1, mult_ra=2)\n",
    "    n3 = ncol()\n",
    "    n4 = ncol(err, n1, n3, 0.5, 0.5)\n",
    "    c5 = ccol(ra_thr=0.5)\n",
    "    c6 = ccol(ra_thr=0.5)\n",
    "    c7 = ccol(err, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=0)\n",
    "    c9 = ccol(err, c6, c8)\n",
    "    t = tfunc(\n",
    "        c7,\n",
    "        c7,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [lin_incr(0.5, -0.5), 0, 0, lin_incr(0.5, -0.5)],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 11000..11999 gradual label\n",
    "    n0 = ncol()\n",
    "    n1 = ncol()\n",
    "    n2 = ncol(err, n1, mult_ra=2)\n",
    "    n3 = ncol()\n",
    "    n4 = ncol(err, n1, n3, 0.5, 0.5)\n",
    "    c5 = ccol(ra_thr=0.5)\n",
    "    c6 = ccol(ra_thr=0.5)\n",
    "    c7 = ccol(err, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=0)\n",
    "    c9 = ccol(err, c6, c8)\n",
    "    grad = gradual(c7, c6)\n",
    "    t = tfunc(\n",
    "        grad,\n",
    "        grad,\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [n2, n3],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [-0.5, 0, 0, -0.5],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    # 12000..12999 sudden label\n",
    "    n0 = ncol()\n",
    "    n1 = ncol()\n",
    "    n2 = ncol(err, n1, mult_ra=2)\n",
    "    n3 = ncol()\n",
    "    n4 = ncol(err, n1, n3, 0.5, 0.5)\n",
    "    c5 = ccol(ra_thr=0.5)\n",
    "    c6 = ccol(ra_thr=0.5)\n",
    "    c7 = ccol(err, c6)\n",
    "    c8 = ccol(err, n3, ra_thr=0)\n",
    "    c9 = ccol(err, c6, c8)\n",
    "    t = tfunc(\n",
    "        c5,\n",
    "        c5,\n",
    "        [n0, n4],\n",
    "        [n0, n4],\n",
    "        [n0, n4],\n",
    "        [n0, n4],\n",
    "        [2, 1],\n",
    "        [2, 1],\n",
    "        [2, 1],\n",
    "        [2, 1],\n",
    "        [1, 0, 0, 1],\n",
    "        [False, False, False, True],\n",
    "        err,\n",
    "    )  # c7:n2<.5, ~c7:n2>.5\n",
    "    tenbatch = np.array([n0, n1, n2, n3, n4, c5, c6, c7, c8, c9, t]).T\n",
    "    dataset = np.concatenate([dataset, tenbatch])\n",
    "\n",
    "    num_cols = [\"n0\", \"n1\", \"n2\", \"n3\", \"n4\"]\n",
    "    cat_cols = [\"c5\", \"c6\", \"c7\", \"c8\", \"c9\"]\n",
    "    label = [\"class\"]\n",
    "    df_dataset = pd.DataFrame(dataset, columns=num_cols + cat_cols + label)\n",
    "    df_dataset[cat_cols] = df_dataset[cat_cols].replace({0: \"a\", 1: \"b\"})\n",
    "    df_dataset[label] = df_dataset[label].replace({0: \"A\", 1: \"B\"})\n",
    "    df_dataset.to_csv(f\"{filename}.csv\", index=False)\n",
    "    df_dataset.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f2dd76-fc55-40b8-8683-49b92cef68af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    error_dists = [\"normal\", \"uniform\"]\n",
    "    corr_funcs = [\"sum\", \"mean\", \"min\", \"max\"]\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Generates a dataset of a correlated feature and noise\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-d\", required=True, type=int, help=\"The no. of instances of the data set\"\n",
    "    )\n",
    "    parser.add_argument(\"-f\", required=True, type=int, help=\"The no. of noise features\")\n",
    "    parser.add_argument(\n",
    "        \"-c\", required=True, type=int, help=\"The no. of correlating features\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-r\", required=True, type=float, help=\"The Â± range of the random component\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--error\",\n",
    "        required=False,\n",
    "        choices=error_dists,\n",
    "        default=error_dists[0],\n",
    "        help=\"The type of error\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cfunc\",\n",
    "        required=False,\n",
    "        choices=corr_funcs,\n",
    "        default=corr_funcs[0],\n",
    "        help=\"The function that fuses the correlated features\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--coffset\",\n",
    "        required=False,\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"A constant offset for correlated features\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ccoeff\",\n",
    "        required=False,\n",
    "        type=float,\n",
    "        default=2.0,\n",
    "        help=\"A factor for correlated features\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cexpo\",\n",
    "        required=False,\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"An exponential for correlated features\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cquant\",\n",
    "        required=False,\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"If set will quantize the correlated feature by multiplying, flooring and dividing with this value\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--filepath\",\n",
    "        required=False,\n",
    "        default=None,\n",
    "        help=\"Specify to set a custom save name\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.f < 0:\n",
    "        raise ValueError(f\"Negative feature values are prohibited (f = {args.f})\")\n",
    "    if args.c < 0:\n",
    "        raise ValueError(f\"Negative feature values are prohibited (c = {args.c})\")\n",
    "    if args.c + args.f == 0:\n",
    "        raise ValueError(\"At least one feature has to be generated\")\n",
    "\n",
    "    folder = (\n",
    "        os.path.join(\"consts.DIR_CSV\", \"corr\", f\"f{args.f}_c{args.c}_r{args.r}\")\n",
    "        if args.filepath is None\n",
    "        else os.path.dirname(args.filepath)\n",
    "    )\n",
    "    filename = (\n",
    "        os.path.join(folder, \"raw_data.pkl.gzip\")\n",
    "        if args.filepath is None\n",
    "        else args.filepath\n",
    "    )\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    noise = np.random.random([args.d, args.f])\n",
    "    corrs = np.random.random([args.d, args.c])\n",
    "\n",
    "    if args.cfunc == corr_funcs[0]:\n",
    "        fcorr = corrs.sum(axis=1, keepdims=True)\n",
    "    elif args.cfunc == corr_funcs[1]:\n",
    "        fcorr = corrs.mean(axis=1, keepdims=True)\n",
    "    elif args.cfunc == corr_funcs[2]:\n",
    "        fcorr = corrs.min(axis=1, keepdims=True)\n",
    "    elif args.cfunc == corr_funcs[3]:\n",
    "        fcorr = corrs.max(axis=1, keepdims=True)\n",
    "\n",
    "    c_true = fcorr**args.cexpo * args.ccoeff + args.coffset\n",
    "\n",
    "    if args.error == error_dists[0]:\n",
    "        c = c_true + np.random.normal(0, args.r, [args.d, 1])\n",
    "    elif args.error == error_dists[1]:\n",
    "        c = c_true + np.random.uniform(-args.r, args.r, [args.d, 1])\n",
    "\n",
    "    if args.cquant != 0:\n",
    "        c = np.floor(c * args.cquant) / args.cquant\n",
    "\n",
    "    label = (c > c_true).reshape([args.d, 1])\n",
    "\n",
    "    cols = (\n",
    "        [f\"N{x}\" for x in range(args.f)] + [f\"C{x}\" for x in range(args.c)] + [\"T\", \"L\"]\n",
    "    )\n",
    "    data = np.concatenate((noise, corrs, c, label), axis=1)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2996e66-e01e-4520-b313-0efc41ed5b3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # filepath = os.path.join(\"consts.DIR_CSV\", \"cfpdss//raw_data.pkl.gzip\")\n",
    "    # change_label_func_splits(filepath)\n",
    "    # df = change_label_func(filepath)\n",
    "    # cfpdss_dataset()\n",
    "    # gen_mixed_dataset()\n",
    "    # main()\n",
    "    []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb4234-7406-4c94-a5fb-94212b244831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
